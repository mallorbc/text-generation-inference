services:
  llm:
    image: ghcr.io/huggingface/text-generation-inference:latest
    ports:
      - "8080:80"          
    volumes:
      - ${HOME}/.cache:/root/.cache
      - ./models:/models
    container_name: llm
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
      - HUGGINGFACE_HUB_CACHE=/root/.cache
    ipc: host
    networks:
      - tgi
    deploy:
          resources:
            reservations:
              devices:
              - driver: "nvidia"
                device_ids: ["0"]
                capabilities: [gpu]
    restart: unless-stopped
    #This is for bitsandbytes quantization
    # command: --model-id meta-llama/Llama-2-7b-hf --huggingface-hub-cache /root/.cache/huggingface/hub  --trust-remote-code --quantize bitsandbytes-nf4 --max-input-length 4095 --max-total-tokens 4096

    #This is for running a custom model with bitsand bytes quantization
    # command: --model-id /models/custom_model --huggingface-hub-cache /root/.cache/huggingface/hub  --trust-remote-code --quantize bitsandbytes-nf4 --max-input-length 4095 --max-total-tokens 4096
    
networks:
  tgi:
    name: tgi-network  